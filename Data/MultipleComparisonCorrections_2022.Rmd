---
title: "Multiple Comparison Corrections"
author: "Jake Butts"
date: "`r Sys.time()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
theme_set(theme_minimal())
set.seed(42)

```

Here we're reading in real data from a study by Kate Schertz. These are topics that people wrote about in park journals, as discovered by topic modeling. There were 29 parks that served as the locations located in and around Baltimore. Each value is the average prevalence of the topic in journal entries for each park.
```{r}

setwd('~/Desktop/Exp Des 1')
parkdata <- read_csv("Study1_ParkEntryAvgs.csv")
head(parkdata)
```

We're going to "pivot longer" to put all the topics in one column for a quick visualization.
```{r}
parkdata_long <- parkdata %>% pivot_longer(cols= -Locations, names_to="Topic", values_to = "Prevalence")

parkdata_long_summarized<-parkdata_long%>%
  group_by(Topic)%>%
  summarise(mean=mean(Prevalence), se=(sd(Prevalence)/sqrt(29)))

#I want my plot to be in the order of my data frame, instead of alphabetical
parkdata_long$Topic <- factor(parkdata_long$Topic, levels=unique(parkdata_long$Topic))

ggplot(data=parkdata_long) + 
  aes(x=Topic, y = Prevalence, color=Topic) + 
  geom_jitter(width = .3) +
  theme(axis.text.x = element_text(angle=45, vjust=.65, hjust = .7))


ggplot(data=parkdata_long)+
  aes(x=Topic,y=Prevalence,color=Topic)+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 45)) 
  
```
Here each dot in our scatter plot represents the average prevalence of the topic in one of the 29 parks. As you can see, their is some interesting variability in the prevalence of each topic in each park (spread of dots in a given color). There also appears to be some variation in the overall prevalence of each topic across parks (center of each topic).The box plot shows this variation in the median Prevalence even more clearly, and also shows that some parks may be considered outliers for certain topics, suggesting that maybe some parks elicit certain topics of reflection much more/less than others.


Today we're going to see if the prevalence of Family (red) is significantly different across all the parks from each of the other topics. Since the topics are coming from the same parks, this is 9 paired t-tests.


```{r}
#Aside: if we wanted to check all topics against all other topics, the total number of tests can be found using the choose() function

t.test(parkdata$Family, parkdata$Park, paired = T)
t.test(parkdata$Family, parkdata$`Life & Emotions`, paired = T)
t.test(parkdata$Family, parkdata$`Time & Memories`, paired = T)
t.test(parkdata$Family, parkdata$Art, paired = T)
t.test(parkdata$Family, parkdata$Nature, paired = T)
t.test(parkdata$Family, parkdata$Religion, paired = T)
t.test(parkdata$Family, parkdata$`World & Peace`, paired = T)
t.test(parkdata$Family, parkdata$Celebration, paired = T)
t.test(parkdata$Family, parkdata$`Spiritual & Life Journey`, paired = T)
```

This is giving us a lot of details but let's make it a little more succinct for easier comparisons. 
```{r}
#Turning off scientific notation
options(scipen=999)

# Saving only the p-values from the t-tests and making them into one vector
nocorrection_p <- as.vector(c(t.test(parkdata$Family, parkdata$Park, paired = T)$p.value,
t.test(parkdata$Family, parkdata$`Life & Emotions`, paired = T)$p.value,
t.test(parkdata$Family, parkdata$`Time & Memories`, paired = T)$p.value,
t.test(parkdata$Family, parkdata$Art, paired = T)$p.value,
t.test(parkdata$Family, parkdata$Nature, paired = T)$p.value,
t.test(parkdata$Family, parkdata$Religion, paired = T)$p.value,
t.test(parkdata$Family, parkdata$`World & Peace`, paired = T)$p.value,
t.test(parkdata$Family, parkdata$Celebration, paired = T)$p.value,
t.test(parkdata$Family, parkdata$`Spiritual & Life Journey`, paired = T)$p.value))

# Saving the topic names in order that the p-value represents. Since I ran the tests in order of columns in the data frame, we can use the column names, removing Locations and Family 
names <- colnames(parkdata)[-(1:2)]

tests_df <- tibble(names, nocorrection_p)
tests_df

# Which are significant with no multiple comparison correction at an alpha of .05?
alpha <- .05
tests_df <- tests_df %>% arrange(nocorrection_p) %>% mutate(SigOrigP = nocorrection_p < alpha)
tests_df
```

So without MCC we would find a significant difference in the prevalence of writing about fa and 7 of the 9 other topics. 

Because we are running NINE t-tests, however, we are inflating the probability of finding a significant result merely by chance. So we should perform a MCC to address our inflated chance of a Type 1 error.


##Bonferroni correction

To perform a Bonferroni correction, we take our p values and compare them to an adjusted alpha level which is defined as our significance level (often 0.5) divided by the number of tests we are performing (here 9)
```{r}
numtests <- nrow(tests_df)
tests_df <- tests_df %>% mutate(SigBonferroniP = nocorrection_p < alpha/numtests)
tests_df
```

With Bonferroni corrected p-values, we would now fail to reject several nulls that would have originally passed our uncorrected significance threshold (p<0.05).This is because our corrected alpha is now approx. 0.005(0.05/9) 

But remember, Bonferroni is a pretty conservative MCC approach. As a result, we may inadvertently be inflating our Type 2 error with this approach-- failing to reject the null when there actually is a significant difference. 

What would happen if we used a less conservative MCC approach like FDR? 


##False Detection Rate (Benjamini-Hochberg procedure)
```{r}
#Since we already ordered the rows by p-value, this is the rank of p-values.
tests_df <- tests_df %>% mutate(i = row_number())

head(tests_df)

#I want to check Q=.05 and Q=.25, but usually you'd just pick one before you ran your study, so you could set Q as a variable
tests_df <- tests_df %>% 
  mutate(BH_q05 = (i/numtests)*.05, q05_sig = nocorrection_p < BH_q05,
         BH_q25 = (i/numtests)*.25, q25_sig = nocorrection_p < BH_q25)
tests_df
```


Let's visualize this to see how different p-values drop out based on the Q you choose.
```{r}
ggplot(data=tests_df) + aes(x=i, y=nocorrection_p) +
  geom_line() +
  geom_line(aes(x=i, y=BH_q05, color="5% FDR"), linetype="longdash") + 
  geom_line(aes(x=i, y=BH_q25, color="25% FDR"), linetype="dashed") +
  scale_x_continuous(breaks=seq(1,9,1)) +
  scale_y_continuous(breaks=seq(0,.35,.05)) +
  labs(x="Rank",y="p/q value", color='q lines')
```

Let's also visualize what happens to the "q lines" when you change the number of tests. Let's say we had tested all topics against all others; that would be 45 t-tests. 
```{r}
tests_df <- tests_df %>% 
  mutate(BH_q05_moretest = (i/45)*.05, q05_moretest_sig = nocorrection_p < BH_q05_moretest,
         BH_q25_moretest = (i/45)*.25, q25_moretest_sig = nocorrection_p < BH_q25_moretest)

ggplot(data=tests_df) + aes(x=i, y=nocorrection_p) +
  geom_line() +
  geom_line(aes(x=i, y=BH_q05, color="5% FDR"), linetype="dotdash") + 
  geom_line(aes(x=i, y=BH_q25, color="25% FDR"), linetype="longdash") +
  geom_line(aes(x=i, y=BH_q05_moretest, color="5% FDR, 45 Tests"), linetype="dashed")+
  geom_line(aes(x=i, y=BH_q25_moretest, color="25% FDR, 45 Tests"), linetype="dotted")+
  scale_x_continuous(breaks=seq(1,9,1)) +
  scale_y_continuous(breaks=seq(0,.35,.05)) +
  labs(x="Rank",y="p/q value", color='q lines')

```

##Using R's built in functions
This function takes a vector of unadjusted p-values and returns the adjusted p-values, which you then compare to your alpha. Instead of dividing alpha by the number of tests, the bonferroni correction is multiplying the p-values by the number of tests. For Benjamini-Hochberg, instead of generating the q-value to compare to your p-values, it's adjusting the p-value directly (p-value*number of test/rank) to compare to your FDR. 
```{r}

tests_df$autoBH <- p.adjust(tests_df$nocorrection_p, method="BH") #BH = Benjamini-Hochberg
tests_df$autoBonferroni <- p.adjust(tests_df$nocorrection_p, method="bonferroni")

head(tests_df$autoBH)
head(tests_df$autoBonferroni)

```

##Extra: Holm-Bonferroni 
(a slightly less conservative version of Bonferroni) 
For the most significant p-value, you divide by the number of tests. For the next p-value, you subtract 1 from the number of tests, because that's the number of tests you have remaining...continuing until you reach a non-significant test. With these data, it didn't make a difference.
```{r}
tests_df <- tests_df %>% mutate(HBsig = nocorrection_p < alpha/(numtests-i+1))
tests_df
```

##Some Final Thoughs

Choosing which MCC approach to take can be tricky. Without correction, you risk a false discovery, but if you over correct you might though the baby out with the bath water, failing to reject a null that you should be rejecting.It is probably best to pre-determine (and pre-register) how you will correct your significance values to limit false positives to reduce the risk of p-hacking with your MCC choice.

It can often be helpful to consider the specific questions you are investigating and whether false positives or false negatives are more consequential. Obviously the goal should be to reduce both as much as possible, but in some cases it might be more costly to over/under correct. Another consideration is what is most commonly done in your field. If you deviate, you should be prepared to explain the method you are using and your reasoning to reviewers/readers.



#Reference: 
Full data set available online: https://doi.org/10.1016/j.cognition.2018.01.011